{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    '''\n",
    "        q:\n",
    "        k:\n",
    "        v:\n",
    "        mask: (batch_size, 1, 1, seq_len) -> in case of encoder should be input seq len and same for q and k\n",
    "    '''\n",
    "    # multiply queries and keys \n",
    "    prod_qk = torch.matmul(q, k.transpose(2, 3)) # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    # NOTE: matmul will do a batched matrix multiplication for N>2 dim\n",
    "    \n",
    "    # scale them by sqrt of depth\n",
    "    scaled_prod_qk = prod_qk / math.sqrt(float(k.shape[-1]))\n",
    "    \n",
    "    # add the mask\n",
    "    if mask is not None:\n",
    "        # for each query, block out the weights associated with keys corresponding to padding tokens or tokens we shouldn't look ahead to\n",
    "        # mask has 1's for tokens to mask, multiply those positions with neg inf and add so q*k prod there becomes neg inf\n",
    "        scaled_prod_qk += (mask * -1e9)\n",
    "        \n",
    "    # take softmax across key dim for each query to get attention weights\n",
    "    attention_weights = nn.functional.softmax(scaled_prod_qk, dim=-1) # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "    # compute weighted sum of keys based on values\n",
    "    output = torch.matmul(attention_weights, v) # seq_len_k == seq_len_v are equal so (batch_size, num_heads, seq_len_q, depth_v)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = self.d_model // self.num_heads # dimension of attention vectors in each head (after splitting across each head)\n",
    "        \n",
    "        self.wq = nn.linear(d_model, d_model) \n",
    "        self.wk = nn.linear(d_model, d_model)\n",
    "        self.wv = nn.linear(d_model, d_model)\n",
    "        \n",
    "        self.dense = nn.linear(d_model, d_model) # brings heads back together\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = torch.reshape(x, (batch_size, -1, self.num_heads, self.depth)) # split last dim into num_heads and depth\n",
    "        x = x.permute(0, 2, 1, 3) # (batch_size, num_heads, seq_len, depth)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, v_inp, k_inp, q_inp, mask):\n",
    "        # compute queries, keys, and values\n",
    "        q = self.wq(q_inp) # (batch_size, seq_len_q, d_model)\n",
    "        k = self.wk(k_inp) # (batch_size, seq_len_k, d_model)\n",
    "        v = self.wv(v_inp) # (batch_size, seq_len_v, d_model)\n",
    "        \n",
    "        # split them across the n heads\n",
    "        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # compute scaled dot prod attention within each head\n",
    "        # attention_outputs.shape = (batch_size, num_heads, seq_len_q, depth_v)\n",
    "        # attention_weights.shape = (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        attention_outputs, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        # bring heads back together\n",
    "        concat_attention_outputs = torch.reshape(attention_weights, (batch_size, -1, self.d_model)) # (batch_size, seq_len_q, depth)\n",
    "        \n",
    "        # pass through final dense layer\n",
    "        output = self.dense(concat_attention_outputs) # (batch_size, seq_len_q, depth)\n",
    "        \n",
    "        return output, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate) # Q: what are dropout and layer norm doing?\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model) # norm across columns\n",
    "\n",
    "        self.feed_fwd_network = nn.Sequential(nn.Linear(d_model, dff), \\\n",
    "            nn.ReLU(), nn.Linear(dff, d_model))\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: (batch size, input_seq_len, d_model)\n",
    "        z = self.multi_head_attention(x, x, x, mask) # (batch size, input_seq_len, d_model)\n",
    "        z = self.dropout1(z)\n",
    "        norm_resid_z = self.layer_norm1(x + z) # (batch size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_out = self.feed_fwd_network(norm_resid_z) # (batch size, input_seq_len, d_model)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "        enc_out = self.layer_norm2(norm_resid_z + ffn_out) # (batch size, input_seq_len, d_model)\n",
    "\n",
    "        return enc_out\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, dropout_rate, input_vocab_size):\n",
    "        \"\"\"\n",
    "            num_layers: number of encoder blocks\n",
    "            attention_vec_dim: \n",
    "            num_heads:\n",
    "            dff:\n",
    "            dropout_rate:\n",
    "            input_vocab_size: \n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(input_vocab_size, attention_vec_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) \\\n",
    "                for _ in range(self.num_layers)]\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x (batch_size, input_seq_len) -> e.g. [1, 12, 21, 17, 0, 0] -> list of ids from tokenizer vocab\n",
    "        x = self.embedding_layer(x)\n",
    "        \n",
    "        #TODO: add positional encodings\n",
    "        \n",
    "        enc_output = self.dropout(x) # Q: why dropout right after embedding, what would that really do?\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            enc_output = self.encoder_layers[i](enc_output, mask)\n",
    "            \n",
    "        return enc_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2])\n",
      "torch.Size([1, 2, 3])\n",
      "3\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.Size([3, 2, 1])\n",
      "torch.Size([1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "mat = torch.zeros(1, 2, 3)\n",
    "mat1 = torch.zeros(1, 3, 2)\n",
    "print(torch.matmul(mat, mat1).shape)\n",
    "print(mat.shape)\n",
    "print(mat.shape[-1])\n",
    "print(mat[-1])\n",
    "print(mat.T.shape)\n",
    "print(mat.permute(0, 2, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
