{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for MR pairs\n",
    "    # Q: why are msg tokens masked in labels? A?: so that the prediction results there are not factored into the loss computation?\n",
    "\n",
    "class MRDataset(Dataset):\n",
    "    def __init__(self, data_path, line_transform):\n",
    "        self.data_path = data_path\n",
    "        self.line_transform = line_transform\n",
    "        \n",
    "        self.data_file = open(self.data_path, 'r', encoding='utf-8') # open data file\n",
    "        self.data_offset_map = [] # element at index if byte offset of that line index\n",
    "        self.create_file_offset()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_offset_map) # offset map has byte offsets for each line\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        offset = self.data_offset_map[idx]\n",
    "        self.data_file.seek(offset, 0)\n",
    "        line = self.data_file.readline()\n",
    "        \n",
    "        # this is where nick applies a series of transforms (wise bc the the series of transforms applied may vary depending on the kind of data the model needs)\n",
    "        return self.line_transform(line) # transform should return msg_ids, rsp_ids in our case\n",
    "        \n",
    "    def create_file_offset(self):\n",
    "        \"\"\"Maps lines in files to the byte offset they're located at in the file, to enable shifting to that byte when reading line\"\"\"\n",
    "        \n",
    "        with open(self.data_path, 'rb') as fh:\n",
    "            # TODO: if already appending a 0 for first line, should for loop start from second line (or do line indexes start from 1)\n",
    "            self.data_offset_map.append(0)  # set the first offset to zero position for a new file\n",
    "            for _ in fh:\n",
    "                # Checks whether we have reached the end of the file or not\n",
    "                # fh.fileno returns the integer id of file_descriptor,\n",
    "                # fstat returns info about the file, and\n",
    "                # st_size gets the file_size in bytes\n",
    "                if not fh.tell() == os.fstat(fh.fileno()).st_size:\n",
    "                    # Adds the current byte offset to the map\n",
    "                    self.data_offset_map.append(fh.tell()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMRIds(object):\n",
    "    def __init__(self, msg_col, reply_col, delimiter, tokenizer):\n",
    "        self.msg_col = msg_col\n",
    "        self.reply_col = reply_col\n",
    "        self.delimiter = delimiter\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, line):\n",
    "        cols = line.split(self.delimiter) # split line into cols based on delimiter\n",
    "        # get message and reply\n",
    "        msg = cols[self.msg_col] \n",
    "        reply = cols[self.reply_col]\n",
    "        \n",
    "        msg_ids = self.tokenizer(msg)['input_ids']\n",
    "        rsp_ids = self.tokenizer(reply)['input_ids']\n",
    "        \n",
    "        return msg_ids, rsp_ids # TODO: does this need to be a tuple explicitly (implicitly already treated as a tuple?)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateMRPairs(object):\n",
    "    def __init__(self, max_msg_len, max_reply_len, pad_token, ignore_token):\n",
    "        self.max_msg_len = max_msg_len\n",
    "        self.max_reply_len = max_reply_len\n",
    "        self.pad_token = pad_token\n",
    "        self.ignore_token = ignore_token\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\" Args:\n",
    "                batch [batch_size, ()]: \n",
    "            return: input_ids, labels \n",
    "        \"\"\"\n",
    "        batch_msg_ids, batch_reply_ids = list(zip(*batch)) # unzip batch list of form [(msg_ids_1, rsp_ids_1)..(msg_ids_2, rsp_ids_2)] to list of msg ids, list of rsp ids\n",
    "        batch_size = len(batch_msg_ids)\n",
    "        \n",
    "        # input ids (batch_size, max_msg_len + max_reply_len) \n",
    "        # (could this technically be max_msg_len + max_reply_len - 1, since last reply token not used for generation?)\n",
    "        # want to pad the input ids vector, in cases where not at max length for msg_ids + reply_ids\n",
    "        input_ids = np.full([batch_size, self.max_msg_len + self.max_reply_len], self.pad_token, dtype=np.long)\n",
    "        print(input_ids.shape)\n",
    "        \n",
    "        # labels (batch_size, max_msg_len + max_reply_len) \n",
    "        # don't care about labels for all msg tokens except last (generates first reply token)\n",
    "        # or for last reply token (no next token to generate)\n",
    "        labels = np.full([batch_size, self.max_msg_len + self.max_reply_len], self.ignore_token, dtype=np.long)\n",
    "        \n",
    "        # for each instance of batch (MR pair), fill in the input_ids and labels to GPT-2 model\n",
    "        for i in range(batch_size):\n",
    "            msg_ids = batch_msg_ids[i]\n",
    "            reply_ids = batch_reply_ids[i]\n",
    "            \n",
    "            msg_ids_len = min(len(msg_ids), max_msg_len)\n",
    "            reply_ids_len = min(len(reply_ids), max_reply_len)\n",
    "            \n",
    "            # tokens 0 ... msg_ids_len - 1 (total msg_ids_len tokens) are taken up by msg_ids\n",
    "            input_ids[i, :msg_ids_len] = msg_ids[:msg_ids_len]\n",
    "            \n",
    "            # tokens msg_ids_len ... msg_ids_len + reply_ids_len - 2 (total reply_ids_len - 1 tokens) are taken up by all reply_ids except last \n",
    "            # (last reply token is not used in generation, since there is no following token)\n",
    "            input_ids[i, msg_ids_len:(msg_ids_len + reply_ids_len - 1)] = reply_ids[:(reply_ids_len - 1)]\n",
    "            \n",
    "            # don't care about labels for any msg token, outside last (don't care about 0-msg_ids_len-2)\n",
    "            # no label for last reply token either (no next token generated)\n",
    "            labels[i, (msg_ids_len - 1):(msg_ids_len - 1 + reply_ids_len)] = reply_ids[:reply_ids_len]\n",
    "            \n",
    "        return input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_path, msg_col, reply_col, delimiter, tokenizer):\n",
    "    line_transform = GetMRIds(msg_col, reply_col, delimiter, tokenizer)\n",
    "    return MRDataset(data_path, line_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(data_path, msg_col, reply_col, delimiter, max_msg_len, max_reply_len, \\\n",
    "          pad_token, ignore_token, batch_size, num_workers):\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # what is the difference between GPT2TokenizerFast vs Tokenizer?\n",
    "    \n",
    "    mr_dataset = create_dataset(data_path, msg_col, reply_col, delimiter, gpt2_tokenizer)\n",
    "    mr_collate_fn = CollateMRPairs(max_msg_len, max_reply_len, pad_token, ignore_token)\n",
    "    mr_dataloader = DataLoader(mr_dataset, batch_size=batch_size, collate_fn=mr_collate_fn, num_workers=num_workers)\n",
    "    \n",
    "    return mr_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/hundred_reddit_mr_pairs.tsv\"\n",
    "msg_col = 0\n",
    "reply_col = 1\n",
    "delimiter = '\\t'\n",
    "max_msg_len = 10\n",
    "max_reply_len = 10\n",
    "pad_token = 0\n",
    "ignore_token = -100\n",
    "batch_size = 10\n",
    "num_workers = 0\n",
    "mr_dataloader = create_dataloader(data_path, msg_col, reply_col, delimiter, max_msg_len, max_reply_len, \\\n",
    "                                  pad_token, ignore_token, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_inputids_labels(max_msg_len, max_rsp_len, pad_token, ignore_token, generated_input_ids, generated_labels):\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    line = \"This works both ways too. If the US wanted to join Canada, we would find a way to accept you too.\tNo thanks.\t180\"\n",
    "    cols = line.split(\"\\t\")\n",
    "    print(\"cols:\", cols)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    msg = cols[0]\n",
    "    reply = cols[1]\n",
    "    print(\"msg:\", msg)\n",
    "    print(\"reply:\", reply)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    msg_ids = gpt2_tokenizer(msg)['input_ids']\n",
    "    reply_ids = gpt2_tokenizer(reply)['input_ids']\n",
    "    print(\"msg_ids:\", msg_ids)\n",
    "    print(\"reply_ids:\", reply_ids, \"\\n\")\n",
    "    \n",
    "    msg_len = min(max_msg_len, len(msg_ids))\n",
    "    reply_len = min(max_reply_len, len(reply_ids))\n",
    "    \n",
    "    print(\"msg_ids truncated by max:\", msg_ids[:max_msg_len])\n",
    "    print(\"reply_ids truncated by max:\", reply_ids[:max_rsp_len], \"\\n\")\n",
    "    \n",
    "    input_ids = np.full(max_msg_len + max_rsp_len, pad_token)\n",
    "    print(\"input ids - initial\\n\", input_ids, \"\\n\")\n",
    "    input_ids[:msg_len] = msg_ids[:msg_len]\n",
    "    print(\"input ids - add in msg ids\\n\", input_ids, \"\\n\")\n",
    "    input_ids[msg_len: (msg_len + reply_len - 1)] = reply_ids[:(reply_len-1)]\n",
    "    print(\"input ids - add in rsp ids (except last)\\n\", input_ids, \"\\n\")\n",
    "    \n",
    "    labels = np.full(max_msg_len + max_rsp_len, ignore_token)\n",
    "    print(\"labels - initial\\n\", labels, \"\\n\")\n",
    "    labels[msg_len-1: msg_len-1+reply_len] = reply_ids[:reply_len]\n",
    "    print(\"labels - adding in reply labels\\n\", labels, \"\\n\")\n",
    "    \n",
    "    assert(np.array_equal(input_ids, generated_input_ids))\n",
    "    assert(np.array_equal(labels, generated_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20)\n",
      "input_ids size: torch.Size([10, 20])\n",
      "labels size: torch.Size([10, 20])\n",
      "logits size: torch.Size([10, 20, 50257])\n",
      "mean loss1: tensor(6.4648, grad_fn=<NllLossBackward>)\n",
      "mean loss2: tensor(6.4648, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▊                                                                                | 1/30 [00:03<01:29,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20)\n",
      "input_ids size: torch.Size([10, 20])\n",
      "labels size: torch.Size([10, 20])\n",
      "logits size: torch.Size([10, 20, 50257])\n",
      "mean loss1: tensor(5.4651, grad_fn=<NllLossBackward>)\n",
      "mean loss2: tensor(5.4651, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▌                                                                             | 2/30 [00:06<01:29,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20)\n",
      "input_ids size: torch.Size([10, 20])\n",
      "labels size: torch.Size([10, 20])\n",
      "logits size: torch.Size([10, 20, 50257])\n",
      "mean loss1: tensor(4.6162, grad_fn=<NllLossBackward>)\n",
      "mean loss2: tensor(4.6162, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 3/30 [00:09<01:29,  3.33s/it]\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    num_epochs = 3\n",
    "    num_training_steps = num_epochs * len(mr_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    \n",
    "    model.train()\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for step, batch in enumerate(mr_dataloader):\n",
    "            input_ids, labels = batch\n",
    "            #if step == 0:\n",
    "                #sanity_check_inputids_labels(10, 10, -100, -100, input_ids[0], labels[0])\n",
    "\n",
    "            input_ids = torch.tensor(input_ids, dtype=torch.long, device=device)\n",
    "            print(\"input_ids size:\", input_ids.shape)\n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "            print(\"labels size:\", labels.shape)\n",
    "\n",
    "            output_dict = model(input_ids)\n",
    "            logits = output_dict[\"logits\"]\n",
    "            print(\"logits size:\", logits.shape)\n",
    "\n",
    "            valid_token_mask = labels != ignore_token\n",
    "            flat_logits1 = logits[valid_token_mask, ...]\n",
    "            flat_labels1 = labels[valid_token_mask]\n",
    "            mean_loss1 = F.cross_entropy(flat_logits1, flat_labels1, ignore_index=-100)\n",
    "            print(\"mean loss1:\", mean_loss1)\n",
    "\n",
    "            # flatten logits across batches \n",
    "            flat_logits2 = logits.view(-1, logits.size(-1))\n",
    "            flat_labels2 = labels.view(-1)\n",
    "            mean_loss2 = F.cross_entropy(flat_logits2, flat_labels2, ignore_index=-100)\n",
    "            print(\"mean loss2:\", mean_loss2)\n",
    "            mean_loss2.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            break\n",
    "\n",
    "        \n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
